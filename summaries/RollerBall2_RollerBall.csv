Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
10000,1.4189383,0.16245544,646.0666666666667,0.06666666666666667,0.06666666666666667,1.0
20000,1.4189641,0.16564466,530.1052631578947,0.05263157894736842,0.05263157894736842,1.0
30000,1.4189897,0.06493196,504.0,0.1,0.1,1.0
40000,1.4189999,0.11678043,654.2,0.13333333333333333,0.13333333333333333,1.0
50000,1.4190005,0.03413678,652.8,0.06666666666666667,0.06666666666666667,1.0
60000,1.4190118,0.0015807899,597.9411764705883,0.058823529411764705,0.058823529411764705,1.0
70000,1.4190176,0.07385248,507.15,0.25,0.25,1.0
80000,1.4190408,0.06649097,442.8181818181818,0.2727272727272727,0.2727272727272727,1.0
90000,1.4190786,0.037152756,512.9473684210526,0.15789473684210525,0.15789473684210525,1.0
100000,1.4191087,0.077487394,468.1818181818182,0.18181818181818182,0.18181818181818182,1.0
110000,1.4191427,0.0077960985,472.85714285714283,0.23809523809523808,0.23809523809523808,1.0
120000,1.4191605,0.06397494,513.3,0.2,0.2,1.0
130000,1.4191675,-0.0060115135,606.1875,0.0,0.0,1.0
140000,1.4191835,0.08201243,590.8125,0.0625,0.0625,1.0
150000,1.4191912,0.021830965,713.1333333333333,0.13333333333333333,0.13333333333333333,1.0
160000,1.4191985,0.12434708,538.8888888888889,0.05555555555555555,0.05555555555555555,1.0
170000,1.4192185,0.13362083,469.45454545454544,0.3181818181818182,0.3181818181818182,1.0
180000,1.4192419,-0.0063439733,663.2666666666667,0.0,0.0,1.0
