Steps,Policy/Entropy,Policy/Extrinsic Value Estimate,Environment/Episode Length,Environment/Cumulative Reward,Policy/Extrinsic Reward,Is Training
10000,1.4189383,-0.33684883,541.1111111111111,0.3888888888888889,0.3888888888888889,1.0
20000,1.4189383,-0.31700712,575.75,0.1875,0.1875,1.0
30000,1.4189383,-0.3775605,901.5833333333334,0.08333333333333333,0.08333333333333333,1.0
40000,1.4189383,-0.5842145,771.4545454545455,0.09090909090909091,0.09090909090909091,1.0
50000,1.4189383,-0.48031092,828.1666666666666,0.25,0.25,1.0
60000,1.4189383,-0.39491087,1035.6363636363637,0.36363636363636365,0.36363636363636365,1.0
70000,1.4189384,-0.2616279,731.2727272727273,0.2727272727272727,0.2727272727272727,1.0
80000,1.4189383,-0.343417,1070.5454545454545,0.09090909090909091,0.09090909090909091,1.0
90000,1.4189384,-0.34251788,836.25,0.16666666666666666,0.16666666666666666,1.0
100000,1.4189383,-0.35933468,672.4,0.3333333333333333,0.3333333333333333,1.0
110000,1.4190902,-0.40857175,590.875,0.1875,0.1875,1.0
120000,1.419096,-0.09505674,625.0,0.17647058823529413,0.17647058823529413,1.0
130000,1.419096,-0.14851205,767.0769230769231,0.07692307692307693,0.07692307692307693,1.0
140000,1.419096,-0.24220084,681.1428571428571,0.21428571428571427,0.21428571428571427,1.0
150000,1.419096,-0.4543611,781.3846153846154,0.15384615384615385,0.15384615384615385,1.0
160000,1.4190959,-0.30036312,702.4285714285714,0.14285714285714285,0.14285714285714285,1.0
170000,1.419096,-0.10828012,923.9090909090909,0.0,0.0,1.0
